version: '3.9'

networks:
  frontend:
      driver: bridge
  backend:
      driver: bridge
  airflow:
      driver: bridge
  mlflow:
      driver: bridge

volumes:
  db_datapg_postresql_mlflow: 
  db_logs_postresql_mlflow: 
  db_datapg_postresql_airflow: 
  db_logs_postresql_airflow: 
  mlrun_data: 
  db_data_mysql:

services:

  db_mysql:
    restart: always
    build: 
      context: ${BACKEND_PROJ_DIR}/databases
    container_name: db_mysql
    volumes:
      - db_data_mysql:/var/lib/mysql
    networks:
      - backend
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      DB_MYSQL_USER: ${DB_MYSQL_USER}
      DB_MYSQL_HOST: ${DB_MYSQL_HOST}
    
  api:
    restart: always
    build:
      context: ${BACKEND_PROJ_DIR}/api
    container_name: api
    environment:
      SECRET_KEY: ${SECRET_KEY}
      ALGORITHM: ${ALGORITHM}
      WEATHER_API_KEY: ${WEATHER_API_KEY}
      DB_ENV: ${DB_ENV}
      FILE_ID: ${FILE_ID}
      ACCESS_TOKEN_EXPIRE_MINUTES: ${ACCESS_TOKEN_EXPIRE_MINUTES}
      BUCKET_NAME: ${BUCKET_NAME}
      S3_URI: ${S3_URI}
      S3_PREFIX: ${S3_PREFIX}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      WAREHOUSE_SNOWFLAKE: ${WAREHOUSE_SNOWFLAKE}
      DB_SNOWFLAKE: ${DB_SNOWFLAKE}
      SCHEMA_SNOWFLAKE: ${SCHEMA_SNOWFLAKE}
      MLFLOW_SERVER_PORT: ${MLFLOW_SERVER_PORT}
      MLFLOW_EXP_NAME: ${MLFLOW_EXP_NAME}
      MODEL_INFERENCE: ${MODEL_INFERENCE}
      FCST_HISTORY: ${FCST_HISTORY}
      FCST_HORIZON: ${FCST_HORIZON}
      URL_HISTORICAL: ${URL_HISTORICAL}
      MUID: ${MUID}
      MGID: ${MGID}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: ${PGDATA}
      BACKEND: ${BACKEND}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      DB_MYSQL_USER: ${DB_MYSQL_USER}
      DB_MYSQL_HOST: ${DB_MYSQL_HOST}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      TERM: ${TERM}
    ports:
      - "8000:8000"
    depends_on:
      - db_mysql
    networks:
        - backend
        - mlflow

  mlflow_postgresql:
    restart: always
    image: postgres:13
    container_name: mlflow_postgresql
    expose:
        - 5432
    environment:
      #PostgreSQL_MLflow
      MUID: ${MUID}
      MGID: ${MGID}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} 
      PGDATA: ${PGDATA}
      BACKEND: ${BACKEND}
    volumes:
        - db_datapg_postresql_mlflow:/var/lib/postgresql/data/pgdata
        - db_logs_postresql_mlflow:/var/lib/postgresql/data/log
    networks:
        - mlflow

  mlflow_server:
    restart: always
    build:
      context: ${BACKEND_PROJ_DIR}/mlflow
    image: mlflow_server
    container_name: mlflow_server
    expose:
        - 5001
    environment:
      MUID: ${MUID}
      MGID: ${MGID}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: ${PGDATA}
      BACKEND: ${BACKEND}
      HOST_MLFLOW_IP: ${HOST_MLFLOW_IP}
      MLFLOW_PORT: ${MLFLOW_PORT}
      NGINX_MLFLOW_PORT: ${NGINX_MLFLOW_PORT}
      MLFLOW_S3_URL: ${MLFLOW_S3_URL}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      ARTIFACTS: ${ARTIFACTS}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
    volumes:
        - mlrun_data:/mlruns
    command: 
        - sh
        - -c
        - mlflow server
            --port $${MLFLOW_PORT}
            --host $${HOST_MLFLOW_IP}
            --backend-store-uri $${BACKEND} 
            --default-artifact-root $${ARTIFACTS}Ã’
            --serve-artifacts
    networks:
        - frontend
        - mlflow
        - backend
    depends_on:
        - db_mysql
        - mlflow_postgresql

  nginx:
    restart: always
    build: 
      context: ${BACKEND_PROJ_DIR}/nginx_proxy
    image: mlflow_nginx
    container_name: mlflow_nginx
    ports:
        - 9000:9000
        - 9001:9001
        - 9002:9002
    networks:
        - frontend
    depends_on:
        - mlflow_server
        - airflow_flower

  airflow_postgresql:
    image: postgres:13
    container_name: airflow_postgresql
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: ${PGDATA}
      MUID: ${MUID}
      MGID: ${MGID}
    expose:
        - 5432
    volumes:
        - /var/run/docker.sock:/var/run/docker.sock
        - db_datapg_postresql_airflow:/var/lib/postgresql/data/pgdata
        - db_logs_postresql_airflow:/var/lib/postgresql/data/log
    command: >
     postgres
       -c listen_addresses=*
       -c logging_collector=on
       -c log_destination=stderr
       -c max_connections=200
    networks:
        - airflow
    restart: always

  airflow_redis:
    image: redis:5.0.5
    container_name: airflow_redis
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      REDIS_HOST: ${REDIS_HOST}
      REDIS_PORT: ${REDIS_PORT}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    ports:
        - 6379:6379
    command:
      - /bin/sh
      - -c
      - redis-server --requirepass $${REDIS_PASSWORD}
    volumes:
        - ${AIRFLOW_PROJ_DIR}/redis-data:/data
    networks:
        - airflow
    restart: always

  airflow_webserver:
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_webserver
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    ports:
        - 8080:8080
    volumes:
        - ${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ${AIRFLOW_PROJ_DIR}/files:/opt/airflow/files
        - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
        - airflow_postgresql
        - airflow_redis
        - airflow_initdb
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 5
    networks:
        - airflow
        - frontend

  airflow_flower:
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_flower
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME}
      AIRFLOW_PWD: ${AIRFLOW_PWD}
    ports:
        - 5555:5555
    depends_on:
        - airflow_redis
        - airflow_initdb
    volumes:
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
    command: celery flower
    networks:
        - airflow
        - backend
        - frontend

  airflow_scheduler:
    #image: apache/airflow:2.5.0-python3.8
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_scheduler
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    volumes:
        - ${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ${AIRFLOW_PROJ_DIR}/files:/opt/airflow/files
        - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    depends_on:
        - airflow_initdb
    restart: always
    networks:
        - airflow
        - backend

  airflow_worker_1:
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_worker_1
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    volumes:
        - ${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ${AIRFLOW_PROJ_DIR}/files:/opt/airflow/files
        - /var/run/docker.sock:/var/run/docker.sock
    command: celery worker -H worker_1_name
    depends_on:
        - airflow_scheduler
        - airflow_initdb
    networks:
        - airflow
        - backend

  airflow_worker_2:
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_worker_2
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    volumes:
        - ${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ${AIRFLOW_PROJ_DIR}/files:/opt/airflow/files
        - /var/run/docker.sock:/var/run/docker.sock
    command: celery worker -H worker_2_name
    depends_on:
        - airflow_scheduler
        - airflow_initdb

    networks:
        - airflow
        - backend

  airflow_initdb:
    build:
      context: ${AIRFLOW_PROJ_DIR}
    container_name: airflow_initdb
    env_file:
      - .env.uid
    user: ${AIRFLOW_UID}:0
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
      AIRFLOW__CORE__CHECK_SLAS: ${AIRFLOW__CORE__CHECK_SLAS}
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: ${AIRFLOW__CORE__STORE_SERIALIZED_DAGS}
      AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: ${AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS}
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: ${AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      LOAD_EX: ${LOAD_EX}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW_LASTNAME: ${AIRFLOW_LASTNAME}
      AIRFLOW_FIRSTNAME: ${AIRFLOW_FIRSTNAME}
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME}
      AIRFLOW_PWD: ${AIRFLOW_PWD}
      AIRFLOW_EMAIL: ${AIRFLOW_EMAIL}
    volumes:
        - ${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ${AIRFLOW_PROJ_DIR}/files:/opt/airflow/files
        - /var/run/docker.sock:/var/run/docker.sock
    # entrypoint: /bin/bash
    command: bash -c 'airflow db init && airflow users create --firstname admin --lastname admin --email admin --password admin --username admin --role Admin'
    depends_on:
        - airflow_redis
        - airflow_postgresql
    networks:
        - airflow