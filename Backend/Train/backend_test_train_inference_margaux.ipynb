{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from tsai.basics import *\n",
    "from tsai.inference import load_learner\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_csv('d:/prog/mlops/projet/data/historical_20080801_full.csv', index_col = 0)\n",
    "\n",
    "df_initial.sort_values(by=['observation_time','time'], inplace = True)  # sort line to have chronoligic order\n",
    "df_initial.reset_index(drop = True, inplace = True)                     # reset of index to able to use split function\n",
    "df_initial.drop(['wind_dir','time','city'], inplace = True, axis = 1)   # remove categorical raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_col = \"observation_time\"\n",
    "freq = 'D'\n",
    "columns = df_initial.columns[1:]\n",
    "method = 'ffill'\n",
    "value = 0\n",
    "\n",
    "# pipeline\n",
    "preproc_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('shrinker', TSShrinkDataFrame()), # shrink dataframe memory usage\n",
    "    ('drop_duplicates', TSDropDuplicates(datetime_col=datetime_col)), # drop duplicate rows (if any)\n",
    "#    ('add_mts', TSAddMissingTimestamps(datetime_col=datetime_col, freq=freq)), # add missing timestamps (if any)\n",
    "    ('fill_missing', TSFillMissing(columns=columns, method=method, value=value)), # fill missing data (1st ffill. 2nd value=0)\n",
    "    ], \n",
    "    verbose=True)\n",
    "mkdir('data', exist_ok=True, parents=True)\n",
    "save_object(preproc_pipe, 'data/preproc_pipe.pkl')\n",
    "preproc_pipe = load_object('data/preproc_pipe.pkl')\n",
    "\n",
    "df = preproc_pipe.fit_transform(df_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fcst_history = 500 # # steps in the past\n",
    "fcst_horizon = 7  # # steps in the future\n",
    "valid_size   = 0.1  # int or float indicating the size of the training set\n",
    "test_size    = 0.2  # int or float indicating the size of the test set\n",
    "\n",
    "splits = get_forecasting_splits(df_initial, fcst_history=fcst_history, fcst_horizon=fcst_horizon, datetime_col=datetime_col,\n",
    "                                valid_size=valid_size, test_size=test_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = splits[0]\n",
    "\n",
    "# pipeline\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=columns)), # standardize data using train_split\n",
    "    ], \n",
    "    verbose=True)\n",
    "save_object(exp_pipe, 'data/exp_pipe.pkl')\n",
    "exp_pipe = load_object('data/exp_pipe.pkl')\n",
    "\n",
    "df_scaled = exp_pipe.fit_transform(df_initial, scaler__idxs=train_split)\n",
    "df_scaled.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = df_initial.columns[1:]\n",
    "y_vars = df_initial.columns[1:]\n",
    "X, y = prepare_forecasting_data(df_initial, fcst_history=fcst_history, fcst_horizon=fcst_horizon, x_vars=x_vars, y_vars=y_vars)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for MacUsers\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_config = dict(\n",
    "    n_layers=3,  # number of encoder layers\n",
    "    n_heads=4,  # number of heads\n",
    "    d_model=16,  # dimension of model\n",
    "    d_ff=128,  # dimension of fully connected network\n",
    "    attn_dropout=0.0, # dropout applied to the attention weights\n",
    "    dropout=0.3,  # dropout applied to all linear layers in the encoder except q,k&v projections\n",
    "    patch_len=24,  # length of the patch applied to the time series to create patches\n",
    "    stride=2,  # stride used when creating patches\n",
    "    padding_patch=True,  # padding_patch\n",
    ")\n",
    "\n",
    "learn = TSForecaster(X, y, splits=splits, batch_size=16, path='d:/prog/mlops/projet/models/', pipelines=[preproc_pipe, exp_pipe],\n",
    "                     arch=\"PatchTST\", arch_config=arch_config, metrics=[mse, mae], cbs=ShowGraph())\n",
    "learn.lr_find().valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Margaux_v0'\n",
    "n_epochs = 20\n",
    "lr_max = 2e-3\n",
    "learn.fit_one_cycle(n_epochs, lr_max=lr_max) # une epoch 20 minutes\n",
    "learn.export('d:/Prog/mlops/projet/models/'+name+'.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Margaux_v0'\n",
    "learn = load_learner('d:/Prog/mlops/projet/models/'+name+'.pt')\n",
    "\n",
    "df_initial = pd.read_csv('d:/prog/mlops/projet/data/historical_20080801_full.csv', index_col = 0)\n",
    "df_initial.sort_values(by=['observation_time','time'], inplace = True)  # sort line to have chronoligic order\n",
    "df_initial.reset_index(drop = True, inplace = True)                     # reset of index to able to use split function\n",
    "df_initial.drop(['wind_dir','time','city'], inplace = True, axis = 1)   # remove categorical raw\n",
    "\n",
    "fcst_history = 500 # # steps in the past\n",
    "fcst_horizon = 7  # # steps in the future\n",
    "valid_size   = 0.1  # int or float indicating the size of the training set\n",
    "test_size    = 0.2  # int or float indicating the size of the test set\n",
    "\n",
    "x_vars = df_initial.columns[1:]\n",
    "y_vars = df_initial.columns[1:]\n",
    "X, y = prepare_forecasting_data(df_initial, fcst_history=fcst_history, fcst_horizon=fcst_horizon, x_vars=x_vars, y_vars=y_vars)\n",
    "X.shape, y.shape\n",
    "\n",
    "datetime_col = \"observation_time\"\n",
    "splits = get_forecasting_splits(df_initial, fcst_history=fcst_history, fcst_horizon=fcst_horizon, datetime_col=datetime_col,\n",
    "                                valid_size=valid_size, test_size=test_size)\n",
    "\n",
    "scaled_preds, *_ = learn.get_X_preds(X[splits[1]])\n",
    "scaled_preds = to_np(scaled_preds)\n",
    "print(f\"scaled_preds.shape: {scaled_preds.shape}\")\n",
    "\n",
    "scaled_y_true = y[splits[1]]\n",
    "results_df = pd.DataFrame(columns=[\"mse\", \"mae\"])\n",
    "results_df.loc[\"valid\", \"mse\"] = mean_squared_error(scaled_y_true.flatten(), scaled_preds.flatten())\n",
    "results_df.loc[\"valid\", \"mae\"] = mean_absolute_error(scaled_y_true.flatten(), scaled_preds.flatten())\n",
    "print('result on Valid samples') \n",
    "display(results_df)\n",
    "\n",
    "y_test_preds, *_ = learn.get_X_preds(X[splits[2]])\n",
    "y_test_preds = to_np(y_test_preds)\n",
    "print(f\"y_test_preds.shape: {y_test_preds.shape}\")\n",
    "\n",
    "y_test = y[splits[2]]\n",
    "results_df = pd.DataFrame(columns=[\"mse\", \"mae\"])\n",
    "results_df.loc[\"test\", \"mse\"] = mean_squared_error(y_test.flatten(), y_test_preds.flatten())\n",
    "results_df.loc[\"test\", \"mae\"] = mean_absolute_error(y_test.flatten(), y_test_preds.flatten())\n",
    "print('result on Test samples') \n",
    "display(results_df)\n",
    "\n",
    "X_test = X[splits[2]]\n",
    "y_test = y[splits[2]]\n",
    "plot_forecast(X_test, y_test, y_test_preds, sel_vars=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
